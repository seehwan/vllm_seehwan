# vLLM 모델 프로파일 설정
# RTX 3090 (24GB VRAM) 기준 최적화, 향후 하드웨어 확장 고려

model_profiles:
  # DeepSeek 모델군 (추천)
  deepseek-r1-distill-qwen-14b:
    name: "DeepSeek R1 Distill Qwen 14B"
    model_id: "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
    description: "최신 DeepSeek R1 distilled 모델, 고성능 추론 및 코딩 작업"
    max_model_len: 8192
    tensor_parallel_size: 2  # 14B 모델이므로 2-GPU 권장
    gpu_memory_utilization: 0.95
    dtype: "float16"
    swap_space: 6
    hardware_requirements:
      min_vram_gb: 30
      recommended_vram_gb: 48
      min_gpus: 2

  deepseek-coder-7b:
    name: "DeepSeek Coder 7B"
    model_id: "deepseek-ai/deepseek-coder-7b-instruct-v1.5"
    description: "코딩 및 프로그래밍 작업에 특화된 모델 (7B)"
    max_model_len: 8192
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.80  # RTX 3090 안정성을 위해 조정
    dtype: "float16"
    swap_space: 4
    hardware_requirements:
      min_vram_gb: 16
      recommended_vram_gb: 24
    
  deepseek-coder-33b:
    name: "DeepSeek Coder 33B"
    model_id: "deepseek-ai/deepseek-coder-33b-instruct"
    description: "고성능 코딩 모델 (33B) - 멀티 GPU 필요"
    max_model_len: 4096
    tensor_parallel_size: 2  # 멀티 GPU 설정
    gpu_memory_utilization: 0.85
    dtype: "float16"
    swap_space: 8
    hardware_requirements:
      min_vram_gb: 40
      recommended_vram_gb: 48
      min_gpus: 2
    
  deepseek-chat-7b:
    name: "DeepSeek Chat 7B"
    model_id: "deepseek-ai/deepseek-llm-7b-chat"
    description: "일반적인 대화 및 질의응답에 최적화된 모델"
    max_model_len: 8192
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.80
    dtype: "float16"
    swap_space: 4
    hardware_requirements:
      min_vram_gb: 16
      recommended_vram_gb: 24

  # Qwen 모델군
  qwen2-7b-instruct:
    name: "Qwen2 7B Instruct"
    model_id: "Qwen/Qwen2-7B-Instruct"
    description: "Alibaba Qwen2 instruction tuned 모델"
    max_model_len: 32768  # 긴 컨텍스트 지원
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.80
    dtype: "float16"
    swap_space: 6
    hardware_requirements:
      min_vram_gb: 18
      recommended_vram_gb: 24

  qwen2-14b-instruct:
    name: "Qwen2 14B Instruct"
    model_id: "Qwen/Qwen2-14B-Instruct"
    description: "더 강력한 Qwen2 14B 모델"
    max_model_len: 16384
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.85
    dtype: "float16"
    swap_space: 8
    hardware_requirements:
      min_vram_gb: 32
      recommended_vram_gb: 40

  # OpenAI OSS 호환 모델들
  llama3-8b-instruct:
    name: "Llama 3 8B Instruct"
    model_id: "meta-llama/Meta-Llama-3-8B-Instruct"
    description: "Meta Llama 3 최신 instruction tuned 모델"
    max_model_len: 8192
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.80
    dtype: "float16"
    swap_space: 4
    hardware_requirements:
      min_vram_gb: 18
      recommended_vram_gb: 24

  llama3-70b-instruct:
    name: "Llama 3 70B Instruct"
    model_id: "meta-llama/Meta-Llama-3-70B-Instruct"
    description: "고성능 Llama 3 70B 모델 - 멀티 GPU 필수"
    max_model_len: 8192
    tensor_parallel_size: 4  # 4-GPU 설정
    gpu_memory_utilization: 0.90
    dtype: "float16"
    swap_space: 16
    hardware_requirements:
      min_vram_gb: 140
      recommended_vram_gb: 160
      min_gpus: 4

  # 경량 모델 (테스트/개발용)
  phi3-mini:
    name: "Phi-3 Mini 3.8B"
    model_id: "microsoft/Phi-3-mini-4k-instruct"
    description: "Microsoft 경량 모델 (개발/테스트용)"
    max_model_len: 4096
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.60
    dtype: "float16"
    swap_space: 2
    hardware_requirements:
      min_vram_gb: 8
      recommended_vram_gb: 12

  # 특수 목적 모델
  codellama-7b:
    name: "Code Llama 7B"
    model_id: "codellama/CodeLlama-7b-Instruct-hf"
    description: "Meta Code Llama 코딩 전용 모델"
    max_model_len: 16384
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.80
    dtype: "float16"
    swap_space: 4
    hardware_requirements:
      min_vram_gb: 16
      recommended_vram_gb: 24

# 하드웨어별 권장 프로파일
hardware_profiles:
  rtx_3090_single:
    name: "RTX 3090 단일 카드"
    vram_gb: 24
    recommended_models:
      - "deepseek-coder-7b"
      - "deepseek-chat-7b"
      - "qwen2-7b-instruct"
      - "llama3-8b-instruct"
      - "phi3-mini"
      - "codellama-7b"
      
  rtx_3090_dual:
    name: "RTX 3090 듀얼 카드"
    vram_gb: 48
    recommended_models:
      - "deepseek-r1-distill-qwen-14b"
      - "deepseek-coder-33b"
      - "qwen2-14b-instruct"
      
  multi_gpu_high_end:
    name: "멀티 GPU 고성능"
    vram_gb: 160
    recommended_models:
      - "llama3-70b-instruct"

default_profile: "deepseek-r1-distill-qwen-14b"
