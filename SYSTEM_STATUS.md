# 🖥️ 시스템 상태 및 실시간 모니터링

## 📅 상태 확인 일시
- **확인 시간**: 2025-01-27 21:39:11 (KST)
- **시스템**: Ubuntu 22.04 LTS
- **GPU**: NVIDIA GeForce RTX 3090 x2
- **드라이버**: NVIDIA-SMI 575.64.03, CUDA 12.9

## 🐳 Docker 컨테이너 상태

| 서비스 | 상태 | 포트 | 헬스체크 |
|--------|------|------|----------|
| **fastapi-gateway** | Up (healthy) | 8080 | ✅ 정상 |
| **nginx-proxy** | Up | 80, 443 | ✅ 정상 |
| **postgres-db** | Up (healthy) | 5432 | ✅ 정상 |
| **react-frontend** | Up | 3000 | ✅ 정상 |
| **redis-cache** | Up (healthy) | 6379 | ✅ 정상 |
| **vllm-server** | Up (healthy) | 8000 | ✅ 정상 |

## 🎮 GPU 사용 현황

### GPU 0 (NVIDIA GeForce RTX 3090)
- **메모리 사용량**: 21,721 MB / 24,576 MB (88.4%)
- **GPU 사용률**: 0%
- **전력 사용량**: 35W / 350W
- **온도**: 40°C
- **프로세스**: python3 (PID: 1511922) - 21,686 MB

### GPU 1 (NVIDIA GeForce RTX 3090)
- **메모리 사용량**: 21,711 MB / 24,576 MB (88.3%)
- **GPU 사용률**: 0%
- **전력 사용량**: 27W / 350W
- **온도**: 38°C
- **프로세스**: /usr/bin/python3 (PID: 1515256) - 21,676 MB

### GPU 분석
- ✅ **모델 로딩 완료**: 두 GPU 모두 약 88% 메모리 사용 중
- ✅ **Tensor Parallel 활성화**: 2개 GPU에 모델 분산 로딩
- ✅ **안정적 온도**: 38-40°C (정상 범위)
- ⚠️ **GPU 사용률 0%**: 현재 추론 요청 대기 상태

## 📊 시스템 리소스

### 메모리 사용량
- **총 메모리**: 251 GiB
- **사용 중**: 10 GiB
- **사용률**: 약 4%
- **상태**: ✅ 여유 공간 충분

### CPU 사용량
- **Load Average**: 1.10
- **상태**: ✅ 정상 범위

### 디스크 사용량
- **상태**: ✅ 충분한 여유 공간

## 🌐 서비스 Health Check 결과

### Gateway API
```json
{
  "status": "healthy",
  "service": "vLLM Chat Gateway", 
  "version": "1.0.0"
}
```

### vLLM API
```json
{
  "object": "list",
  "data": [
    {
      "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "object": "model",
      "created": 1759023364,
      "owned_by": "vllm",
      "max_model_len": 4096
    }
  ]
}
```

## 🔧 모델 관리 시스템 상태

### 현재 실행 중인 모델
- **모델 ID**: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
- **상태**: running
- **설정**: Tensor Parallel 2, GPU 메모리 사용률 95%
- **VRAM 사용량**: 43.4GB (2 GPU 합계)

### 지원 모델 목록 (10개)
1. **DeepSeek R1 Distill Qwen 14B** ⭐ (현재 실행)
2. DeepSeek Coder 7B
3. DeepSeek Coder 33B
4. DeepSeek Chat 7B
5. Qwen2 7B Instruct
6. Qwen2 14B Instruct
7. Llama 3 8B Instruct
8. Llama 3 70B Instruct
9. Phi-3 Mini 3.8B
10. Code Llama 7B

## 🔐 인증 시스템 상태

### 개발용 계정
- **admin**: admin123 (활성화됨)
- **test**: test (활성화됨)
- **dev**: dev (활성화됨)

### JWT 토큰 상태
- **알고리즘**: HS256
- **만료 시간**: 30분
- **상태**: ✅ 정상 작동

## 📈 성능 메트릭

### API 응답 시간 (최근 측정)
- **헬스체크**: < 100ms
- **모델 상태 조회**: < 200ms
- **로그인**: < 150ms
- **채팅 응답**: ~4초 (첫 토큰)

### 처리량
- **동시 연결**: 최대 20개 지원
- **RPS**: 20+ 요청/초 처리 가능
- **스트리밍**: SSE 실시간 응답 지원

## 🚨 알림 및 경고

### 현재 경고 사항
- ⚠️ **GPU 메모리 사용률 높음**: 88% (정상 범위 내이지만 모니터링 필요)
- ℹ️ **GPU 사용률 0%**: 현재 추론 요청 없음 (정상)

### 권장 모니터링
- GPU 메모리 사용률 추적
- 모델 전환 시 메모리 정리 확인
- 장시간 운영 시 온도 모니터링

## 🔄 자동화 명령어

### 시스템 관리
```bash
# 전체 상태 확인
./status

# 서비스 시작
./start

# 서비스 정지  
./stop

# 서비스 재시작
./restart

# 로그 확인
./logs [서비스명]
```

### 성능 테스트
```bash
# 벤치마크 실행
./scripts/benchmark.sh

# 부하 테스트
k6 run k6/load-test.js
```

### 모니터링
```bash
# GPU 상태 실시간 모니터링
nvidia-smi -l 5

# 시스템 리소스 모니터링
htop
```

## 📋 운영 체크리스트

### 일일 점검 항목
- [x] 모든 컨테이너 상태 확인
- [x] GPU 메모리 사용률 확인
- [x] API 헬스체크 수행
- [x] 로그 파일 확인
- [x] 디스크 공간 확인

### 주간 점검 항목
- [ ] 성능 벤치마크 실행
- [ ] 로그 파일 정리
- [ ] 데이터베이스 백업
- [ ] 시스템 업데이트 확인

### 월간 점검 항목
- [ ] 보안 패치 적용
- [ ] 성능 최적화 검토
- [ ] 용량 계획 수립
- [ ] 문서 업데이트

## 🎯 성능 최적화 권장사항

### GPU 메모리 최적화
- 현재 88% 사용률로 안정적
- 모델 전환 시 메모리 정리 확인 필요
- 필요시 VLLM_UTIL 조정 고려

### API 성능 최적화
- 응답 시간 4초는 14B 모델 기준 정상
- 스트리밍 모드 사용 권장
- 캐싱 전략 고려

### 시스템 안정성
- 정기적인 로그 모니터링
- GPU 온도 추적
- 메모리 누수 확인

## 📞 지원 및 문제 해결

### 긴급 연락처
- **시스템 관리자**: [연락처]
- **개발팀**: [연락처]
- **인프라팀**: [연락처]

### 문제 해결 가이드
1. **서비스 다운**: `./restart` 실행
2. **GPU 메모리 부족**: 모델 전환 또는 재시작
3. **응답 지연**: 로그 확인 및 리소스 점검
4. **인증 오류**: 토큰 갱신 또는 재로그인

---

**문서 업데이트**: 2025-01-27 21:39:11  
**다음 업데이트 예정**: 2025-01-28  
**상태**: 모든 시스템 정상 운영 중 ✅
