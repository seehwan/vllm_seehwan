version: '3.9'

services:
  vllm:
    image: vllm/vllm-openai:v0.5.0
    container_name: vllm-server
    restart: "no"  # 모델 전환 시 수동 재시작을 위해 변경
    ipc: host
    shm_size: '2gb'
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models
      - ./model_profiles.yml:/app/model_profiles.yml:ro
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - NCCL_DEBUG=INFO
      - CUDA_VISIBLE_DEVICES=0,1
    command: >
      --model ${MODEL_ID}
      --dtype ${VLLM_DTYPE:-float16}
      --max-model-len ${VLLM_MAXLEN}
      --tensor-parallel-size ${VLLM_TP}
      --gpu-memory-utilization ${VLLM_UTIL}
      --swap-space ${VLLM_SWAP_SPACE}
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    healthcheck:
      test: ['CMD', 'python3', '-c', 'import urllib.request; urllib.request.urlopen("http://localhost:8000/v1/models")']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  gateway:
    build: ./gateway
    container_name: fastapi-gateway
    restart: unless-stopped
    ports:
      - '${GATEWAY_PORT}:8080'
    volumes:
      - ./model_profiles.yml:/app/model_profiles.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker 컨테이너 제어를 위해 추가
      - /usr/bin/nvidia-smi:/usr/bin/nvidia-smi:ro  # nvidia-smi 바이너리
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.575.64.03:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro  # NVIDIA ML 라이브러리
    devices:
      - /dev/nvidiactl:/dev/nvidiactl        # NVIDIA 제어 디바이스
      - /dev/nvidia0:/dev/nvidia0            # GPU 0 디바이스
      - /dev/nvidia1:/dev/nvidia1            # GPU 1 디바이스
      - /dev/nvidia-modeset:/dev/nvidia-modeset  # NVIDIA modeset 디바이스
      - /dev/nvidia-uvm:/dev/nvidia-uvm      # NVIDIA UVM 디바이스
      - /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools  # NVIDIA UVM tools
    environment:
      - VLLM_BASE_URL=${VLLM_BASE_URL}
      - JWT_SECRET=${JWT_SECRET}
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      - REDIS_URL=${REDIS_URL}
      - LOG_LEVEL=${LOG_LEVEL}
      - DEBUG=${DEBUG}
      - ENVIRONMENT=${ENVIRONMENT}
    depends_on:
      - vllm
      - postgres
      - redis
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8080/health']
      interval: 30s
      timeout: 10s
      retries: 3

  frontend:
    build: ./frontend
    container_name: react-frontend
    restart: unless-stopped
    ports:
      - '${FRONTEND_PORT}:3000'
    environment:
      - VITE_API_BASE_URL=http://localhost:${GATEWAY_PORT}
    depends_on:
      - gateway

  postgres:
    image: postgres:16-alpine
    container_name: postgres-db
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - '5432:5432'
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}']
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: redis-cache
    restart: unless-stopped
    command: redis-server --save "" --appendonly no
    ports:
      - '6379:6379'
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 10s
      timeout: 3s
      retries: 3

  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    restart: unless-stopped
    ports:
      - '${NGINX_PORT:-80}:80'
      - '${NGINX_SSL_PORT:-443}:443'
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - frontend
      - gateway

volumes:
  postgres_data:

networks:
  default:
    name: vllm_network
