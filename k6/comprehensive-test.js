import http from 'k6/http'
import { check, sleep } from 'k6'
import { Rate, Trend, Counter } from 'k6/metrics'

// ì¢…í•© ì„±ëŠ¥ ë©”íŠ¸ë¦­
const errorRate = new Rate('errors')
const chatResponseTime = new Trend('chat_response_time', true)
const tokenThroughput = new Counter('tokens_generated')
const apiCallCounter = new Counter('api_calls')

export const options = {
  scenarios: {
    // ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸°ë³¸ ì±„íŒ… ë¶€í•˜ í…ŒìŠ¤íŠ¸
    chat_load: {
      executor: 'ramping-vus',
      startVUs: 0,
      stages: [
        { duration: '1m', target: 2 },  // ì›Œë°ì—…
        { duration: '3m', target: 5 },  // ì •ìƒ ë¶€í•˜
        { duration: '2m', target: 8 },  // í”¼í¬ ë¶€í•˜
        { duration: '1m', target: 0 },  // ì¢…ë£Œ
      ],
      exec: 'chatLoadTest',
    },
    
    // ì‹œë‚˜ë¦¬ì˜¤ 2: ìŠ¤íŠ¸ë¦¬ë° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    streaming_test: {
      executor: 'constant-vus',
      vus: 2,
      duration: '5m',
      exec: 'streamingTest',
    },
    
    // ì‹œë‚˜ë¦¬ì˜¤ 3: API ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
    api_stability: {
      executor: 'per-vu-iterations',
      vus: 1,
      iterations: 10,
      maxDuration: '10m',
      exec: 'apiStabilityTest',
    },
  },
  
  thresholds: {
    // ì „ì²´ ì„±ëŠ¥ ëª©í‘œ
    http_req_duration: ['p(50)<2000', 'p(95)<5000', 'p(99)<10000'],
    http_req_failed: ['rate<0.05'], // 5% ë¯¸ë§Œ
    errors: ['rate<0.05'],
    
    // ì±„íŒ… íŠ¹í™” ë©”íŠ¸ë¦­
    chat_response_time: ['p(50)<3000', 'p(95)<8000'],
    
    // ì²˜ë¦¬ëŸ‰ ëª©í‘œ (ë¶„ë‹¹ ìµœì†Œ ìš”ì²­ ìˆ˜)
    api_calls: ['count>=100'],
  },
}

// ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë³„ í”„ë¡¬í”„íŠ¸
const prompts = {
  simple: [
    'ì•ˆë…•í•˜ì„¸ìš”!',
    'How are you?', 
    'ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œìš”?',
    'Thank you!',
  ],
  
  medium: [
    'Pythonìœ¼ë¡œ ê°„ë‹¨í•œ ì›¹ í¬ë¡¤ëŸ¬ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.',
    'Explain the difference between machine learning and deep learning.',
    'ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì˜ ì¥ë‹¨ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.',
    'What are the best practices for REST API design?',
  ],
  
  complex: [
    'Write a comprehensive guide on implementing microservices architecture with Docker and Kubernetes, including best practices for monitoring and scaling.',
    'Create a detailed comparison of different database types (relational, document, graph, time-series) and explain when to use each one.',
    'ëŒ€ê·œëª¨ ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œ ë°ì´í„° ì¼ê´€ì„±ì„ ë³´ì¥í•˜ëŠ” ë°©ë²•ë“¤ì„ ë¹„êµ ë¶„ì„í•˜ê³ , CAP ì •ë¦¬ì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.',
  ],
}

export function setup() {
  console.log('ğŸš€ ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘')
  
  // í…ŒìŠ¤íŠ¸ í™˜ê²½ ê²€ì¦
  const healthCheck = http.get('http://localhost:8080/health')
  if (healthCheck.status !== 200) {
    throw new Error('Gateway ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
  }
  
  const modelStatus = http.get('http://localhost:8080/api/models/status')
  if (modelStatus.status !== 200) {
    throw new Error('ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
  }
  
  const status = JSON.parse(modelStatus.body)
  console.log(`í…ŒìŠ¤íŠ¸ ëª¨ë¸: ${status.current_profile}`)
  console.log(`GPU ìƒíƒœ: ${JSON.stringify(status.hardware_info?.gpus?.[0] || {}, null, 2)}`)
  
  return { 
    testToken: 'test-token',
    initialModel: status.current_profile,
  }
}

// ì‹œë‚˜ë¦¬ì˜¤ 1: ì±„íŒ… ë¶€í•˜ í…ŒìŠ¤íŠ¸
export function chatLoadTest(data) {
  apiCallCounter.add(1)
  
  // í”„ë¡¬í”„íŠ¸ ë³µì¡ë„ë³„ ëœë¤ ì„ íƒ
  let selectedPrompt
  const complexity = Math.random()
  
  if (complexity < 0.4) {
    // 40%: ê°„ë‹¨í•œ ì§ˆë¬¸
    selectedPrompt = prompts.simple[Math.floor(Math.random() * prompts.simple.length)]
  } else if (complexity < 0.8) {
    // 40%: ì¤‘ê°„ ë³µì¡ë„
    selectedPrompt = prompts.medium[Math.floor(Math.random() * prompts.medium.length)]
  } else {
    // 20%: ë³µì¡í•œ ì§ˆë¬¸
    selectedPrompt = prompts.complex[Math.floor(Math.random() * prompts.complex.length)]
  }

  const payload = JSON.stringify({
    messages: [{ role: 'user', content: selectedPrompt }],
    model: 'current',
    temperature: 0.7,
    max_tokens: complexity > 0.8 ? 500 : 200, // ë³µì¡í•œ ì§ˆë¬¸ì¼ ë•Œ ë” ê¸´ ì‘ë‹µ
    stream: false,
  })

  const startTime = Date.now()
  const response = http.post('http://localhost:8080/api/chat', payload, {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${data.testToken}`,
    },
    timeout: '60s',
  })
  
  const responseTime = Date.now() - startTime
  chatResponseTime.add(responseTime)

  const success = check(response, {
    'chat status 200': (r) => r.status === 200,
    'has valid response': (r) => {
      try {
        const body = JSON.parse(r.body)
        return body.choices && body.choices[0] && body.choices[0].message
      } catch (e) {
        return false
      }
    },
    'response time acceptable': () => responseTime < 30000, // 30ì´ˆ ì´ë‚´
  })

  if (!success) {
    errorRate.add(1)
    console.error(`ì±„íŒ… ìš”ì²­ ì‹¤íŒ¨: ${response.status} - ${response.body?.substring(0, 100)}`)
  } else {
    errorRate.add(0)
    
    // í† í° ìˆ˜ ì¶”ì • (ì‹¤ì œë¡œëŠ” ì‘ë‹µì—ì„œ íŒŒì‹±í•´ì•¼ í•¨)
    try {
      const body = JSON.parse(response.body)
      if (body.usage && body.usage.completion_tokens) {
        tokenThroughput.add(body.usage.completion_tokens)
      }
    } catch (e) {
      // í† í° ìˆ˜ë¥¼ ì¶”ì •í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
    }
  }

  sleep(Math.random() * 2 + 1) // 1-3ì´ˆ ëŒ€ê¸°
}

// ì‹œë‚˜ë¦¬ì˜¤ 2: ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
export function streamingTest(data) {
  apiCallCounter.add(1)
  
  const payload = JSON.stringify({
    messages: [{ 
      role: 'user', 
      content: 'ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì™€ í˜„ì¬ ê¸°ìˆ  ë™í–¥ì— ëŒ€í•´ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ì¥ë‹¨ì ì„ í¬í•¨í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.'
    }],
    model: 'current',
    temperature: 0.8,
    max_tokens: 400,
    stream: true,
  })

  const response = http.post('http://localhost:8080/api/chat', payload, {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${data.testToken}`,
      'Accept': 'text/event-stream',
    },
    timeout: '90s',
  })

  const streamingSuccess = check(response, {
    'streaming status 200': (r) => r.status === 200,
    'streaming content type': (r) => r.headers['content-type']?.includes('event-stream') || r.body?.includes('data:'),
    'has streaming data': (r) => r.body && r.body.length > 0,
  })

  if (!streamingSuccess) {
    errorRate.add(1)
    console.error(`ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: ${response.status}`)
  } else {
    errorRate.add(0)
  }

  sleep(3) // ìŠ¤íŠ¸ë¦¬ë°ì€ ë” ê¸´ ê°„ê²©
}

// ì‹œë‚˜ë¦¬ì˜¤ 3: API ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
export function apiStabilityTest(data) {
  const apis = [
    { url: '/health', method: 'GET', name: 'health' },
    { url: '/api/models/status', method: 'GET', name: 'models_status' },
    { url: '/api/models/profiles', method: 'GET', name: 'models_profiles' },
    { url: '/api/models/hardware-recommendations', method: 'GET', name: 'hardware_rec' },
  ]

  apis.forEach(api => {
    apiCallCounter.add(1)
    
    const response = http.get(`http://localhost:8080${api.url}`)
    
    const success = check(response, {
      [`${api.name} responds`]: (r) => r.status === 200,
      [`${api.name} response time`]: (r) => r.timings.duration < 5000,
    })

    if (!success) {
      errorRate.add(1)
      console.error(`API ${api.name} ì‹¤íŒ¨: ${response.status}`)
    } else {
      errorRate.add(0)
    }
    
    sleep(0.5) // API ê°„ ì§§ì€ ëŒ€ê¸°
  })

  sleep(2) // ë¼ìš´ë“œ ê°„ ëŒ€ê¸°
}

export function teardown(data) {
  console.log('ğŸ“Š ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ')
  
  // ìµœì¢… ìƒíƒœ ë¦¬í¬íŠ¸
  const finalStatus = http.get('http://localhost:8080/api/models/status')
  if (finalStatus.status === 200) {
    const status = JSON.parse(finalStatus.body)
    console.log(`ìµœì¢… ëª¨ë¸ ìƒíƒœ: ${status.current_profile} (${status.status})`)
    
    if (status.hardware_info?.gpus) {
      status.hardware_info.gpus.forEach((gpu, i) => {
        console.log(`GPU ${i}: ${gpu.memory_used_mb}MB / ${gpu.memory_total_mb}MB (${Math.round(gpu.memory_used_mb/gpu.memory_total_mb*100)}%)`)
      })
    }
  }
}
